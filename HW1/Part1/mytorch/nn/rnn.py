import numpy as np
from mytorch import tensor
from mytorch.tensor import Tensor
from mytorch.nn.module import Module
from mytorch.nn.activations import Tanh, ReLU, Sigmoid
from mytorch.nn.util import PackedSequence, pack_sequence, unpack_sequence


class RNNUnit(Module):
    '''
    This class defines a single RNN Unit block.

    Args:
        input_size (int): # features in each timestep
        hidden_size (int): # features generated by the RNNUnit at each timestep
        nonlinearity (string): Non-linearity to be applied the result of matrix operations 
    '''

    def __init__(self, input_size, hidden_size, nonlinearity='tanh'):

        super(RNNUnit, self).__init__()

        # Initializing parameters
        self.weight_ih = Tensor(np.random.randn(hidden_size, input_size), requires_grad=True, is_parameter=True)
        self.bias_ih = Tensor(np.zeros(hidden_size), requires_grad=True, is_parameter=True)
        self.weight_hh = Tensor(np.random.randn(hidden_size, hidden_size), requires_grad=True, is_parameter=True)
        self.bias_hh = Tensor(np.zeros(hidden_size), requires_grad=True, is_parameter=True)

        self.hidden_size = hidden_size

        # Setting the Activation Unit
        if nonlinearity == 'tanh':
            self.act = Tanh()
        elif nonlinearity == 'relu':
            self.act = ReLU()

    def __call__(self, input, hidden=None):
        return self.forward(input, hidden)

    def forward(self, input, hidden=None):
        '''
        Args:
            input (Tensor): (effective_batch_size,input_size)
            hidden (Tensor,None): (effective_batch_size,hidden_size)
        Return:
            Tensor: (effective_batch_size,hidden_size)
        '''

        # TODO: INSTRUCTIONS
        # Perform matrix operations to construct the intermediary value from input and hidden tensors
        # Apply the activation on the resultant
        # Remeber to handle the case when hidden = None. Construct a tensor of appropriate size, filled with 0s to use as the hidden.
        weighted_input = input @ self.weight_ih.T()
        if hidden is None:
            hidden = Tensor(np.zeros((input.shape[0], self.weight_ih.shape[0])))
        weighted_hidden = hidden @ self.weight_hh.T()
        self.act = Tanh()
        return self.act(weighted_input + self.bias_ih.reshape(1, -1) + weighted_hidden + self.bias_hh.reshape(1, -1))


class TimeIterator(Module):
    '''
    For a given input this class iterates through time by processing the entire
    seqeunce of timesteps. Can be thought to represent a single layer for a 
    given basic unit which is applied at each time step.
    
    Args:
        basic_unit (Class): RNNUnit or GRUUnit. This class is used to instantiate the unit that will be used to process the inputs
        input_size (int): # features in each timestep
        hidden_size (int): # features generated by the RNNUnit at each timestep
        nonlinearity (string): Non-linearity to be applied the result of matrix operations 

    '''

    def __init__(self, basic_unit, input_size, hidden_size, nonlinearity='tanh'):
        super(TimeIterator, self).__init__()

        # basic_unit can either be RNNUnit or GRUUnit
        self.unit = basic_unit(input_size, hidden_size, nonlinearity)

    def __call__(self, input, hidden=None):
        return self.forward(input, hidden)

    def forward(self, input, hidden=None):
        '''
        NOTE: Please get a good grasp on util.PackedSequence before attempting this.

        Args:
            input (PackedSequence): input.data is tensor of shape ( total number of timesteps (sum) across all samples in the batch, input_size)
            hidden (Tensor, None): (batch_size, hidden_size)
        Returns:
            PackedSequence: ( total number of timesteps (sum) across all samples in the batch, hidden_size)
            Tensor (batch_size,hidden_size): This is the hidden generated by the last time step for each sample joined together. Samples are ordered in descending order based on number of timesteps. This is a slight deviation from PyTorch.
        '''

        # Resolve the PackedSequence into its components
        data, sorted_indices, batch_sizes = input

        # TODO: INSTRUCTIONS
        # Iterate over appropriate segments of the "data" tensor to pass same timesteps across all samples in the batch simultaneously to the unit for processing.
        # Remeber to account for scenarios when effective_batch_size changes between one iteration to the next
        timesteps = []
        start = 0
        for batch_size in batch_sizes:
            # current_timestep = data[start: start + int(batch_size)]
            current_timestep = tensor.cat(
                [data[i].reshape(-1, data.shape[1]) for i in range(start, start + int(batch_size))], 0)
            start += int(batch_size)
            timesteps.append(current_timestep)
        hidden_outputs = []
        hidden = self.unit(timesteps[0], hidden=None)
        hidden_outputs.append(hidden)
        for timestep in timesteps[1:]:
            hidden = self.unit(timestep, tensor.cat([hidden[i].reshape(1, -1) for i in range(timestep.shape[0])], 0))#hidden[:timestep.shape[0]])
            hidden_outputs.append(hidden)
        unpacked = unpack_sequence(input)
        lengths = sorted([sample.shape[0] for sample in unpacked], reverse=True)
        if len(unpacked) == 1:
            last_hidden_out = hidden_outputs[-1]
        else:
            last_hidden_steps = [hidden_outputs[length - 1][-1] for length in lengths]
            last_hidden_out = tensor.cat(last_hidden_steps, 0)
        hidden_out = tensor.cat(hidden_outputs, 0)
        hidden_ps = PackedSequence(data=hidden_out, sorted_indices=sorted_indices, batch_sizes=batch_sizes)
        return hidden_ps, last_hidden_out.reshape(len(unpacked), -1)


class RNN(TimeIterator):
    '''
    Child class for TimeIterator which appropriately initializes the parent class to construct an RNN.
    Args:
        input_size (int): # features in each timestep
        hidden_size (int): # features generated by the RNNUnit at each timestep
        nonlinearity (string): Non-linearity to be applied the result of matrix operations 
    '''

    def __init__(self, input_size, hidden_size, nonlinearity='tanh'):
        # TODO: Properly Initialize the RNN class
        super(RNN, self).__init__(RNNUnit, input_size, hidden_size, nonlinearity)
