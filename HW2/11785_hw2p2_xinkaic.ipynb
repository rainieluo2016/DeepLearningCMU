{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "11785-hw2p2",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "REfsKAxVJhGQ"
      },
      "source": [
        "!pip install kaggle\n",
        "!mkdir .kaggle\n",
        "import json\n",
        "token = {\"username\":\"xinkaichen97\",\"key\":\"a31b1965f229c3f21b0282be2d9cdd3f\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "!chmod 600 /content/.kaggle/kaggle.json\n",
        "!cp /content/.kaggle/kaggle.json /root/.kaggle/\n",
        "!kaggle config path -p /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2i3uWwLJ8uk"
      },
      "source": [
        "!pip install ipdb\n",
        "import os, zipfile, tarfile, ipdb\n",
        "os.environ['KAGGLE_USERNAME'] = \"xinkaichen97\" \n",
        "os.environ['KAGGLE_KEY'] = \"a31b1965f229c3f21b0282be2d9cdd3f\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT1dmNZ7KCyb"
      },
      "source": [
        "!kaggle datasets download -d cmu11785/20fall-hw2p2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joRLZ8tCoKNs"
      },
      "source": [
        "!unzip 20fall-hw2p2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22D5XLXOeF1C"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import ImageFolder\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from collections import OrderedDict\n",
        "from torch import Tensor\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiXhKyohpYZG"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on the CPU\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yyygk3RIRqhi"
      },
      "source": [
        "# data augmentation\n",
        "aug_transforms_1 = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomHorizontalFlip(p=1),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "])\n",
        "aug_transforms_2 = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "    torchvision.transforms.RandomRotation(20, resample=Image.BILINEAR),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkwC3CXSR85V"
      },
      "source": [
        "transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtiF7xxvSHlt"
      },
      "source": [
        "train_original = ImageFolder('classification_data/train_data', transform=transforms)\n",
        "train_augmented_1 = ImageFolder('classification_data/train_data', transform=aug_transforms_1)\n",
        "train_augmented_2 = ImageFolder('classification_data/train_data', transform=aug_transforms_2)\n",
        "train_loader = torch.utils.data.ConcatDataset([train_original, train_augmented_1, train_augmented_2])\n",
        "print(len(train_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EydLZGjueSje"
      },
      "source": [
        "# train_loader = ImageFolder('classification_data/train_data', transform=ToTensor())\n",
        "val_loader = ImageFolder('classification_data/val_data', transform=ToTensor())\n",
        "test_loader = ImageFolder('classification_data/test_data', transform=ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEIjxR9-edzK"
      },
      "source": [
        "print(len(train_loader))\n",
        "print(train_loader[580637][0].shape, train_loader[580637][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOruCYsjD80n"
      },
      "source": [
        "print(len(val_loader))\n",
        "print(len(test_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyE5Lva8mQv0"
      },
      "source": [
        "batch_train = DataLoader(train_loader, batch_size=200, shuffle=True, num_workers=4)\n",
        "batch_val = DataLoader(val_loader, batch_size=200, shuffle=False, num_workers=4)\n",
        "batch_test = DataLoader(test_loader, batch_size=200, shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlDrJORuiGkJ"
      },
      "source": [
        "# baseline\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2),)\n",
        "        self.layer3 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2),)\n",
        "        self.layer4 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        self.layer5 = nn.Sequential(nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2),)\n",
        "        self.layer6 = nn.Sequential(nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        self.layer7 = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2),)\n",
        "        self.layer8 = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2),)\n",
        "        self.layer9 = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2),)\n",
        "        # self.layer10 = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(),)\n",
        "        # self.layer11 = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(),)\n",
        "        # self.layer12 = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.AvgPool2d(kernel_size=2, stride=2),)\n",
        "        self.layer13 = nn.Flatten(start_dim=1)\n",
        "        self.fc = nn.Linear(256, 4000, bias=False)\n",
        "        # self.residual = nn.Identity()\n",
        "  \n",
        "  def forward(self, x):\n",
        "        y = self.layer1(x)\n",
        "        y = self.layer2(y)\n",
        "        y = self.layer3(y)\n",
        "        y = self.layer4(y)\n",
        "        y = self.layer5(y)\n",
        "        y = self.layer6(y)\n",
        "        y = self.layer7(y)\n",
        "        y = self.layer8(y)\n",
        "        y = self.layer9(y)\n",
        "        # y = self.layer10(y)\n",
        "        # y = self.layer11(y)\n",
        "        # y = self.layer12(y)\n",
        "        y = self.layer13(y)\n",
        "        classification = self.fc(y)\n",
        "        return y, classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yHt9IJnjMV5"
      },
      "source": [
        "# similar to VGG-19 \n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64), nn.ReLU(),)\n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64), nn.ReLU(),)\n",
        "        self.layer3 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2),)\n",
        "        self.layer4 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(128), nn.ReLU(),)\n",
        "        self.layer5 = nn.Sequential(nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(128), nn.ReLU(),)\n",
        "        self.layer6 = nn.Sequential(nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2),)\n",
        "        self.layer7 = nn.Sequential(nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(),)\n",
        "        self.layer8 = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(),)\n",
        "        self.layer9 = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2),)\n",
        "        self.layer10 = nn.Sequential(nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(),)\n",
        "        self.layer11 = nn.Sequential(nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(),)\n",
        "        self.layer12 = nn.Sequential(nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2),)\n",
        "        self.layer13 = nn.Sequential(nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(1024), nn.ReLU(),)\n",
        "        self.layer14 = nn.Sequential(nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(1024), nn.ReLU(),)\n",
        "        self.layer15 = nn.Sequential(nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(1024), nn.ReLU(),)\n",
        "        self.layer16 = nn.Sequential(nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(1024), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2),)\n",
        "        self.layer17 = nn.Sequential(nn.Conv2d(1024, 2048, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(2048), nn.ReLU(), )\n",
        "        self.layer18= nn.Sequential(nn.Conv2d(2048, 2048, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(2048), nn.ReLU(),)\n",
        "        self.layer19 = nn.Sequential(nn.Conv2d(2048, 2048, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(2048), nn.ReLU(),)\n",
        "        self.layer20 = nn.Sequential(nn.Conv2d(2048, 1024, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(1024), nn.ReLU(),)\n",
        "        self.layer21 = nn.Sequential(nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(), nn.AvgPool2d(kernel_size=2, stride=2),)\n",
        "        self.layer22 = nn.Flatten(1)\n",
        "        self.layer23 = nn.Linear(512, 4000)\n",
        "        # self.linear_closs = nn.Linear(2048, 2048, bias=False)\n",
        "        # self.relu_closs = nn.ReLU(inplace=True)\n",
        "  \n",
        "  def forward(self, x):\n",
        "        y = self.layer1(x)\n",
        "        y = self.layer2(y)\n",
        "        y = self.layer3(y)\n",
        "        y = self.layer4(y)\n",
        "        y = self.layer5(y)\n",
        "        y = self.layer6(y)\n",
        "        y = self.layer7(y)\n",
        "        y = self.layer8(y)\n",
        "        y = self.layer9(y)\n",
        "        y = self.layer10(y)\n",
        "        y = self.layer11(y)\n",
        "        y = self.layer12(y)\n",
        "        y = self.layer13(y)\n",
        "        y = self.layer14(y)\n",
        "        y = self.layer15(y)\n",
        "        y = self.layer16(y)\n",
        "        y = self.layer17(y)\n",
        "        y = self.layer18(y)\n",
        "        y = self.layer19(y)\n",
        "        y = self.layer20(y)\n",
        "        y = self.layer21(y)\n",
        "        y = self.layer22(y)\n",
        "        # only include Linear layer when training\n",
        "        y = self.layer23(y)        \n",
        "        # return closs_out, y\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed7xSfmARXBw"
      },
      "source": [
        "# ResNet\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.layer0 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "        self.layer1 = self.res_layer(block, layers[0], intermediate_channels=64, stride=1)\n",
        "        self.layer2 = self.res_layer(block, layers[1], intermediate_channels=128, stride=2)\n",
        "        self.layer3 = self.res_layer(block, layers[2], intermediate_channels=256, stride=2)\n",
        "        self.layer4 = self.res_layer(block, layers[3], intermediate_channels=512, stride=2)\n",
        "        self.layer5 = nn.Sequential(nn.Conv2d(2048, 1024, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(1024), nn.ReLU())\n",
        "        self.layer6 = nn.Sequential(nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU())\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * 4, num_classes, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x = self.layer6(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def res_layer(self, block, num_residual_blocks, out_channels, stride):\n",
        "        downsample = None\n",
        "        layers = []\n",
        "\n",
        "        if stride != 1 or self.in_channels != out_channels * 4:\n",
        "            downsample = nn.Sequential(nn.Conv2d(self.in_channels, out_channels * 4, kernel_size=1, stride=stride), nn.BatchNorm2d(out_channels * 4))\n",
        "        layers.append(ResidualBlock(self.in_channels, out_channels, downsample, stride))\n",
        "\n",
        "        self.in_channels = out_channels * 4\n",
        "\n",
        "        for i in range(num_residual_blocks - 1):\n",
        "            layers.append(ResidualBlock(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample=None):\n",
        "        super(block, self).__init__()\n",
        "        self.expansion = 4\n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), nn.BatchNorm2d(out_channels), nn.ReLU())\n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(out_channels), nn.ReLU())\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0), nn.BatchNorm2d(out_channels * self.expansion))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(identity)\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def ResNet50(num_classes=4000):\n",
        "    return ResNet(block, [3, 4, 6, 3], num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePkn6cu2OjtI"
      },
      "source": [
        "class CenterLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        num_classes (int): number of classes.\n",
        "        feat_dim (int): feature dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, feat_dim):\n",
        "        super(CenterLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.feat_dim = feat_dim\n",
        "        \n",
        "        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: feature matrix with shape (batch_size, feat_dim).\n",
        "            labels: ground truth labels with shape (batch_size).\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
        "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
        "        distmat.addmm_(1, -2, x, self.centers.t())\n",
        "\n",
        "        classes = torch.arange(self.num_classes).long().cuda()\n",
        "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
        "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
        "\n",
        "        dist = []\n",
        "        for i in range(batch_size):\n",
        "            value = distmat[i][mask[i]]\n",
        "            value = value.clamp(min=1e-12, max=1e+12) # for numerical stability\n",
        "            dist.append(value)\n",
        "        dist = torch.cat(dist)\n",
        "        loss = dist.mean()\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV1305upl15l"
      },
      "source": [
        "model = ResNet50().cuda()\n",
        "criterion_label = nn.CrossEntropyLoss()\n",
        "# criterion_closs = CenterLoss(4000, 2048)\n",
        "optimizer_label = torch.optim.SGD(model.parameters(), lr=0.15, weight_decay=5e-5, momentum=0.9)\n",
        "# optimizer_closs = torch.optim.SGD(criterion_closs.parameters(), lr=0.5)\n",
        "# closs_weight = 0.001\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer_label, step_size=1, gamma=0.85)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBfEhzdzpgHn"
      },
      "source": [
        "# validation\n",
        "def validate(model, batch_val):\n",
        "    num_correct = 0\n",
        "    for i, (batch_data, batch_labels) in enumerate(batch_val):\n",
        "            batch_labels = batch_labels.cuda()\n",
        "            batch_data = batch_data.cuda()\n",
        "            # print(batch_labels.shape)\n",
        "            outputs = model(batch_data)\n",
        "            pred = torch.argmax(outputs, dim=1)\n",
        "            current_correct = len([pred[i] for i in range(len(pred)) if pred[i] == batch_labels[i]])\n",
        "            num_correct += current_correct\n",
        "    # print(num_correct, len(batch_val))\n",
        "    accuracy = num_correct / (len(batch_val) * 200)\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5y8KcmApf0E"
      },
      "source": [
        "# training\n",
        "val_accuracies = []\n",
        "test_accuracies = []\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (batch_data, batch_labels) in enumerate(batch_train):\n",
        "        batch_labels = batch_labels.cuda()\n",
        "        batch_data = batch_data.cuda()\n",
        "        running_loss = 0.0\n",
        "        optimizer_label.zero_grad()\n",
        "        # optimizer_closs.zero_grad()\n",
        "        outputs = model(batch_data)\n",
        "        l_loss = criterion_label(outputs, batch_labels.long())\n",
        "        loss = l_loss\n",
        "        l_loss.backward()\n",
        "        optimizer_label.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print('[%d, %5d] loss: %.10f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "    scheduler.step() \n",
        "    val_accu = validate(model, batch_val)\n",
        "    test_accu = validate(model, batch_test)\n",
        "    print(val_accu, test_accu)\n",
        "    val_accuracies.append(val_accu)\n",
        "    test_accuracies.append(test_accu)\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuLbyDYkZYnL"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(range(1, 8), val_accuracies)\n",
        "plt.plot(range(1, 8), test_accuracies)\n",
        "plt.plot(range(1, 8), scores)\n",
        "plt.ylabel('Accuracy on validation & Test')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rq7UVTx1SYj"
      },
      "source": [
        "torch.save(model.state_dict(), \"model_res50_aug_e18.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YuEu9nwuHb7",
        "outputId": "e1156fd4-da69-4a80-ccef-5055141c7580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.load_state_dict(torch.load('model_res50_aug_e13.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CV2UzvE_WrK"
      },
      "source": [
        "val_accu = validate(model, batch_val)\n",
        "test_accu = validate(model, batch_test)\n",
        "print(val_accu, test_accu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKS3g0dc_gys"
      },
      "source": [
        "# just a small test\n",
        "first_class = [i for i in range(100) if train_loader[i][1] == 0]\n",
        "twentieth_class = [i for i in range(2000) if train_loader[i][1] == 20]\n",
        "similarity1 = []\n",
        "similarity2 = []\n",
        "embedding0, _ = model(train_loader[0][0].unsqueeze(0).cuda())\n",
        "for i in first_class:\n",
        "  embedding, _ = model(train_loader[i][0].unsqueeze(0).cuda())\n",
        "  similarity1.append(cos_sim(embedding0, embedding).cpu().detach().numpy().item())\n",
        "for i in twentieth_class:\n",
        "  embedding, _ = model(train_loader[i][0].unsqueeze(0).cuda())\n",
        "  similarity2.append(cos_sim(embedding0, embedding).cpu().detach().numpy().item())\n",
        "print(min(similarity1), max(similarity1), len(similarity1), similarity1)\n",
        "print(min(similarity2), max(similarity2), len(similarity2), similarity2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzMhXajMFDPs"
      },
      "source": [
        "# !mkdir verification\n",
        "!cp -r ./verification_data ./verification/verification_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7MLgnbp8CUr"
      },
      "source": [
        "# one way of loading verification data (that is not used)\n",
        "veri_loader = torchvision.datasets.ImageFolder('verification/', transform=torchvision.transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwzbBfuSDVse"
      },
      "source": [
        "print(len(veri_loader))\n",
        "print(veri_loader[0], veri_loader[0][0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7OXvGwZKTgM"
      },
      "source": [
        "def img_to_tensor(img_path):\n",
        "  image = Image.open(img_path)\n",
        "  image_tensor = ToTensor()(image).unsqueeze(0).cuda()\n",
        "  return image_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOvmxY6CHtJJ",
        "outputId": "a9e67e0b-7ddd-4179-a000-4b4433fd0ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# validation\n",
        "with open('verification_pairs_val.txt', 'r') as f:\n",
        "   val_labels = []\n",
        "   val_imgs = []\n",
        "   for line in f.readlines():\n",
        "     line = line.strip('\\n')\n",
        "     fields = line.split(\" \")\n",
        "     val_imgs.append((fields[0], fields[1]))\n",
        "     val_labels.append(int(fields[2]))\n",
        "print(len(val_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkrbgA9Ol_Iq"
      },
      "source": [
        "scores = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG4nOsvXKA8q"
      },
      "source": [
        "val_similarities = []\n",
        "for img1, img2 in val_imgs:\n",
        "  embedding1 = model(img_to_tensor(img1))\n",
        "  embedding2 = model(img_to_tensor(img2))\n",
        "  val_similarities.append(torch.cosine_similarity(embedding1, embedding2).cpu().detach().numpy().item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPFzLBYHjT9g"
      },
      "source": [
        "score = roc_auc_score(val_labels, val_similarities)\n",
        "print(score)\n",
        "scores.append(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlnTetHdV52U",
        "outputId": "3825304b-08a2-4261-eada-8594c33235fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('verification_pairs_test.txt', 'r') as f:\n",
        "  #  labels = []\n",
        "   test_imgs = []\n",
        "   for line in f.readlines():\n",
        "     line = line.strip('\\n')\n",
        "     fields = line.split(\" \")\n",
        "     test_imgs.append((fields[0], fields[1]))\n",
        "print(len(test_imgs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v0h4A00WK83"
      },
      "source": [
        "# submission\n",
        "with open('submission.csv', 'w') as f:\n",
        "  f.write('Id,Category\\n')\n",
        "  for img1, img2 in test_imgs:\n",
        "    f.write(img1 + \" \" + img2 + \",\")\n",
        "    embedding1 = model(img_to_tensor(img1))\n",
        "    embedding2 = model(img_to_tensor(img2))\n",
        "    cos = torch.cosine_similarity(embedding1, embedding2).cpu().detach().numpy().item()\n",
        "    f.write(str(cos) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3-MXHl7Ws5B"
      },
      "source": [
        "!kaggle competitions submit -c 11-785-fall-20-homework-2-part-2 -f submission.csv -m \"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}